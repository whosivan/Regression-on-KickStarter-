{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        links = f.readlines()\n",
    "\n",
    "    url_list = []\n",
    "    for link in links:\n",
    "        link = link.split(\"\\n\")[0]\n",
    "        url_list.append(link)\n",
    "    \n",
    "    return url_list\n",
    "\n",
    "def wrap_str(text):\n",
    "    if '\"' in text:\n",
    "        text = text.replace('\"', '\"\"')\n",
    "    return '\"' + text + '\"'\n",
    "\n",
    "def clean_comma(text):\n",
    "    return text.replace(',', '')\n",
    "\n",
    "def clean_money(text):\n",
    "    return re.sub('[SE|U|£|$| |,|CA|NZ|Â£|â‚¬|MX|HK|F|O|D]', '', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filename = 'Dataset/final_urls.txt'\n",
    "url_list = open_data('../Dataset/final_urls.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "successful_url = 'https://www.kickstarter.com/projects/914635363/lace-anchors-20-a-simple-design-for-simple-people'\n",
    "live_url = 'https://www.kickstarter.com/projects/487334780/living-in-a-dream'\n",
    "fail_url = 'https://www.kickstarter.com/projects/1579473521/re-sail-bags-hand-crafted-in-redondo-beach-from-us'\n",
    "testurl = [successful_url, live_url, fail_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def failed_project(soup):\n",
    "\n",
    "    title = str(soup.find('title')\\\n",
    "                    .text\\\n",
    "                    .split('by')[0]\\\n",
    "                    .strip())\n",
    "    inventor = re.split(r\"[—K|— K]\", str(soup.find('title')\\\n",
    "                   .text\\\n",
    "                   .split('by')[1]))[1]\\\n",
    "                   .strip()\n",
    "    number_of_backers = str(soup.find_all('div', attrs = {'id': 'backers_count'})[0]\\\n",
    "                            .text.strip())\n",
    "    total_pledged = str(soup.find('div', attrs={'class':'num nowrap'})['data-pledged']\\\n",
    "                        .split('.')[0])\n",
    "    goal = str(soup.find(attrs={'class':'num nowrap'})['data-goal'].split('.')[0])\n",
    "    location = str(soup.find_all('a', href=re.compile(r'/discover/places/'))[-1]\\\n",
    "                   .text.strip()\\\n",
    "                   .replace(', ', '-'))                \n",
    "    category = str(soup.find_all('a', href=re.compile(r'/discover/categories/'))[-1]\\\n",
    "                   .text\\\n",
    "                   .strip())\n",
    "    number_of_pledged_option = str(len(soup.find_all('li', class_='pledge-selectable-sidebar')))\n",
    "\n",
    "    all_categories = soup.find_all('li', class_='pledge-selectable-sidebar')#not being scraped for project\n",
    "    Pledge_Detail = [(category.find('span', class_='pledge__backer-count').text.strip(), \n",
    "                      category.find('span', class_='money').text.strip()) for category in all_categories]\n",
    "    Pledge_Detail_str = ', '.join(map(str, Pledge_Detail))\n",
    "    \n",
    "    return [wrap_str(title), \n",
    "            inventor, \n",
    "            clean_comma(number_of_backers), \n",
    "            clean_money(total_pledged), \n",
    "            clean_money(clean_comma(goal)), \n",
    "            location, \n",
    "            category, \n",
    "            number_of_pledged_option,\n",
    "            wrap_str(Pledge_Detail_str), 'fail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def live_project(soup):\n",
    "\n",
    "    title = str(soup.find('title')\\\n",
    "                    .text\\\n",
    "                    .split('by')[0]\\\n",
    "                    .strip())\n",
    "    inventor = re.split(r\"[—K|— K]\", str(soup.find('title')\\\n",
    "                   .text\\\n",
    "                   .split('by')[1]))[1]\\\n",
    "                   .strip()\n",
    "    number_of_backers = str(soup.find_all('div', attrs = {'id': 'backers_count'})[0]\\\n",
    "                            .text.strip())\n",
    "    total_pledged = str(soup.find('div', attrs={'class':'num nowrap'})['data-pledged']\\\n",
    "                        .split('.')[0])\n",
    "    try:\n",
    "        goal_text = soup.find(\"div\", attrs={\"class\":\"NS_campaigns__spotlight_stats\"})\\\n",
    "                   .find('span', attrs={'class':'money'}).text\n",
    "    except:\n",
    "        goal_text = soup.find(\"div\", attrs={\"class\":\"NS_campaigns__stats\"})\\\n",
    "                   .find('span', attrs={'class':'money'}).text\n",
    "        \n",
    "    location = str(soup.find_all('a', href=re.compile(r'/discover/places/'))[-1]\\\n",
    "                   .text.strip()\\\n",
    "                   .replace(', ', '-'))                \n",
    "    category = str(soup.find_all('a', href=re.compile(r'/discover/categories/'))[-1]\\\n",
    "                   .text\\\n",
    "                   .strip())\n",
    "    number_of_pledged_option = str(len(soup.find_all('li', class_='pledge-selectable-sidebar')))\n",
    "\n",
    "    all_categories = soup.find_all('li', class_='pledge-selectable-sidebar')#not being scraped for project\n",
    "    Pledge_Detail = [(category.find('span', class_='pledge__backer-count').text.strip(), \n",
    "                      category.find('span', class_='money').text.strip()) for category in all_categories[1:]]\n",
    "    Pledge_Detail_str = ', '.join(map(str, Pledge_Detail))\n",
    "    \n",
    "    return [wrap_str(title), \n",
    "            inventor, \n",
    "            clean_comma(number_of_backers), \n",
    "            clean_money(total_pledged), \n",
    "            clean_money(clean_comma(goal)), \n",
    "            location, \n",
    "            category, \n",
    "            number_of_pledged_option,\n",
    "            wrap_str(Pledge_Detail_str), 'live'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def success_project(soup):\n",
    "\n",
    "    title = str(soup.find('title')\\\n",
    "                .text\\\n",
    "                .split('by')[0]\\\n",
    "                .strip())  \n",
    "    inventor = re.split(r\"[—K|— K]\", str(soup.find('title')\\\n",
    "                   .text\\\n",
    "                   .split('by')[1]))[1]\\\n",
    "                   .strip()  \n",
    "    number_of_backers = str(soup.find_all('b')[0]\\\n",
    "                            .text.split(' ')[0])\n",
    "    total_pledged = str(soup.find_all('b')[0]\\\n",
    "                        .find_next()\\\n",
    "                        .text\\\n",
    "                        .replace('$', '')\\\n",
    "                        .replace(',', ''))\n",
    "    \n",
    "    goal_text = (soup.find(\"div\", attrs={\"class\":\"NS_projects__description_section\"})\n",
    "                 .find(\"div\", attrs={\"class\":\"description-container\"})\n",
    "                 .find_all(\"span\", attrs={\"class\":\"money\"})[1]\n",
    "                 .text\n",
    "                )\n",
    "    goal = clean_money(goal_text)\n",
    "    \n",
    "    location = str(soup.find_all('a', href=re.compile(r'/discover/places/'))[-1]\\\n",
    "                   .text.strip()\\\n",
    "                   .replace(', ', '-'))                \n",
    "    category = str(soup.find_all('a', href=re.compile(r'/discover/categories/'))[-1]\\\n",
    "                   .text\\\n",
    "                   .strip())\n",
    "    number_of_pledged_option = str(len(soup.find_all('li', class_='pledge-selectable-sidebar')))\n",
    "\n",
    "    all_categories = soup.find_all('li', class_='pledge-selectable-sidebar')#not being scraped for project\n",
    "    Pledge_Detail = [(category.find('span', class_='pledge__backer-count').text.strip(), \n",
    "                      category.find('span', class_='money').text.strip()) for category in all_categories]\n",
    "    Pledge_Detail_str = ', '.join(map(str, Pledge_Detail))\n",
    "    \n",
    "    return [wrap_str(title), \n",
    "            inventor, \n",
    "            clean_comma(number_of_backers), \n",
    "            clean_money(total_pledged), \n",
    "            clean_money(clean_comma(goal)), \n",
    "            location, \n",
    "            category, \n",
    "            number_of_pledged_option,\n",
    "            wrap_str(Pledge_Detail_str), 'success']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(url_list):\n",
    "    with open('KS_data_test.csv', 'a+') as data:\n",
    "        data.write(','.join(['title', \n",
    "                             'inventor', \n",
    "                             'number_of_backers', \n",
    "                             'total_pledged', \n",
    "                             'goal', \n",
    "                             'location', \n",
    "                             'category', \n",
    "                             'number_of_pledged_option',\n",
    "                             'Pledge_Detail']) + '\\n')\n",
    "\n",
    "    for i in range(len(url_list)):\n",
    "        try:\n",
    "            print(\"Link\", i)\n",
    "            response = requests.get(url_list[i]).text\n",
    "            soup = bs(response, 'html5lib')\n",
    "\n",
    "            status_successful = soup.find('div', attrs = {'class':'Campaign-state-successful'})\n",
    "            status_live = soup.find('div', attrs = {'class':'Campaign-state-live'})\n",
    "            status_failed = soup.find('div', attrs = {'class':'Campaign-state-failed'})\n",
    "\n",
    "            if status_successful is None:\n",
    "                pass\n",
    "            else:\n",
    "                result = success_project(soup)\n",
    "\n",
    "            if status_live is None:\n",
    "                pass\n",
    "            else:\n",
    "                result = live_project(soup)\n",
    "\n",
    "            if status_failed is None:\n",
    "                pass\n",
    "            else:\n",
    "                result = failed_project(soup)\n",
    "\n",
    "            with open('KS_data_test.csv', 'a+') as data:\n",
    "                data.write(','.join([result[0], result[1], result[2], result[3], result[4], result[5], result[6], result[7], result[8], result[9]]) + '\\n')\n",
    "        except:\n",
    "            with open(\"failed_links.csv\", \"a+\") as data:\n",
    "                data.write(str(i) + \",\" + wrap_str(url_list[i]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def get_single_data(url):\n",
    "    try:\n",
    "        response = requests.get(url).text\n",
    "        soup = bs(response, \"html5lib\")\n",
    "\n",
    "        status_successful = soup.find('div', attrs = {'class':'Campaign-state-successful'})\n",
    "        status_live = soup.find('div', attrs = {'class':'Campaign-state-live'})\n",
    "        status_failed = soup.find('div', attrs = {'class':'Campaign-state-failed'})\n",
    "\n",
    "        if status_successful is None:\n",
    "            pass\n",
    "        else:\n",
    "            result = success_project(soup)\n",
    "        \n",
    "        if status_live is None:\n",
    "            pass\n",
    "        else:\n",
    "            result = live_project(soup)\n",
    "\n",
    "        if status_failed is None:\n",
    "            pass\n",
    "        else:\n",
    "            result = failed_project(soup)\n",
    "\n",
    "        with open('KickStarter_data.csv', 'a+') as data:\n",
    "            data.write(','.join([result[0], result[1], result[2], \n",
    "                                 result[3], result[4], result[5], \n",
    "                                 result[6], result[7], result[8], result[9]]) + '\\n')\n",
    "    except:\n",
    "        with open('failed_links.csv', 'a+') as data:\n",
    "            data.write(\",\" + wrap_str(url) + \"\\n\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    pool = Pool(20)\n",
    "    pool.map(get_single_data, url_list[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Tag'> <class 'NoneType'> <class 'NoneType'>\n",
      "['\"Imaginary Drugs\"', 'Michael', '650', '13609', '5500', 'Barnegat-NJ', 'Comics', '20', '\"(\\'13 backers\\', \\'$1\\'), (\\'164 backers\\', \\'$5\\'), (\\'278 backers\\', \\'$10\\'), (\\'40 backers\\', \\'$15\\'), (\\'78 backers\\', \\'$25\\'), (\\'2 backers\\', \\'$30\\'), (\\'37 backers\\', \\'$40\\'), (\\'6 backers\\', \\'$50\\'), (\\'11 backers\\', \\'$65\\'), (\\'1 backer\\', \\'$100\\'), (\\'3 backers\\', \\'$150\\'), (\\'5 backers\\', \\'$150\\'), (\\'0 backers\\', \\'$150\\'), (\\'3 backers\\', \\'$150\\'), (\\'0 backers\\', \\'$150\\'), (\\'1 backer\\', \\'$150\\'), (\\'1 backer\\', \\'$200\\'), (\\'1 backer\\', \\'$250\\'), (\\'1 backer\\', \\'$250\\'), (\\'1 backer\\', \\'$300\\')\"', 'success']\n"
     ]
    }
   ],
   "source": [
    "get_single_data(url_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link 0\n",
      "Link 1\n",
      "Link 2\n",
      "Link 3\n",
      "Link 4\n",
      "Link 5\n",
      "Link 6\n",
      "Link 7\n",
      "Link 8\n",
      "Link 9\n",
      "Link 10\n",
      "Link 11\n",
      "Link 12\n",
      "Link 13\n",
      "Link 14\n",
      "Link 15\n",
      "Link 16\n",
      "Link 17\n",
      "Link 18\n",
      "Link 19\n"
     ]
    }
   ],
   "source": [
    "get_data(url_list[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
